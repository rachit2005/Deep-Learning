# -*- coding: utf-8 -*-
"""text generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q_0Wk6Mq_jXeqztuGRoKANmTLNhUXY1q
"""

import torch
from torch import nn
import string
import random
import sys
import unidecode
from torch.utils.tensorboard import SummaryWriter

all_characters = string.printable
num_charact = len(all_characters)

# reading the file
file = unidecode.unidecode(open('/content/English.txt').read())

file

class RNN(nn.Module):
  def __init__(self , input_size , hidden_size , num_layers , output_size):
    super().__init__()

    self.hidden_size = hidden_size
    self.num_layers = num_layers

    # https://medium.com/@gautam.e/what-is-nn-embedding-really-de038baadd24
    self.embed = nn.Embedding(input_size , hidden_size) # allows the network to learn more about the relationship between inputs and process data more efficiently
    self.lstm = nn.LSTM(hidden_size , hidden_size , num_layers , batch_first = True)
    self.fc = nn.Linear(hidden_size , output_size)

  def forward(self , x , hidden , cell):
    # print(f"x shape : {x.shape}")
    out = self.embed(x) # print and then see what it does
    # print(f"after embedding shape {out.shape}")

    # LSTM: Expected input to be 2D or 3D tensor
    out,(hidden , cell) = self.lstm(out.unsqueeze(1) , (hidden , cell))
    # print(f"after lstm {out.shape}")
    out = self.fc(out.reshape(out.size(0) , -1))
    # print(f"after linear layer {out.shape}")
    return out , (hidden , cell)

  def init_hidden(self , batch_size):
     # making a hidden state and cell state for lstm model of size (num_layers , batch_size of x , hidden_size)
    hidden0 = torch.zeros(self.num_layers , batch_size , self.hidden_size)
    cell0 = torch.zeros(self.num_layers , batch_size , self.hidden_size)

    return hidden0 , cell0


model = RNN(num_charact , 128 , 2 , num_charact)

class Generator():
  def __init__(self):
    self.chunk_len = 250 # how many characters generator is gonna take at a time
    self.num_epoch = 500
    self.batch_size = 1
    self.print_every = 10
    self.hidden_size = 256
    self.num_layer = 2
    self.lr = 0.003
    # initialising the model
    self.rnn = RNN(num_charact , self.hidden_size , self.num_layer , num_charact)

  def char_tensor(self , string):
    '''convert any sentence to a tensor'''
    tensor = torch.zeros(len(string)).long()
    for c in range(len(string)):
      tensor[c] =all_characters.index(string[c]) # replce tensor'zeros of that position to character's index position number

    return tensor

  def get_random_batch(self):
    start_idx = random.randint(0 , len(file) - self.chunk_len - 1)
    end_idx = start_idx + self.chunk_len + 1
    text_str = file[start_idx:end_idx] # getting the text from start_idx to end_idx
    text_tensor = self.char_tensor(text_str) # converting the text to its tensor representation

    text_input = torch.zeros(self.batch_size , self.chunk_len)
    text_target = torch.zeros(self.batch_size , self.chunk_len)

    for i in range(self.batch_size):
      text_input[i,:] = self.char_tensor(text_str[:-1])
      text_target[i,:] = self.char_tensor(text_str[1:self.chunk_len+1])

    return text_input.long() , text_target.long()


  def train(self):
    '''train the RNN model to generate names'''
    optimizer = torch.optim.SGD(self.rnn.parameters() , lr = self.lr)
    criterion = nn.CrossEntropyLoss()
    writer = SummaryWriter(f'runs/names0') # for tensor board

    # training the model
    print("starting training ")
    for epoch in range(1, self.num_epoch + 1):
      hidden , cell = self.rnn.init_hidden(self.batch_size)

      inp , tar = self.get_random_batch()
      loss = 0
      optimizer.zero_grad()
      # print(inp.shape) # [ batch_size , chunk_len]
      # print(tar.shape) # [ batch_size , chunk_len]

      for c in range(self.chunk_len):
        output , (hidden , cell) = self.rnn(inp[:,c] , hidden , cell)
        # print(output.shape) # [1,100] --> [batch_size , num_charact]
        loss += criterion(output , tar[: , c])
        # after going through all character
      loss.backward()
      optimizer.step()
      loss = loss.item()/self.chunk_len

      if epoch% self.print_every == 0:
        print(f"epoch {epoch} || loss {loss}")
        print(self.generate())

      writer.add_scalar('training loss' , loss , global_step = epoch)

  def generate(self , initial_str ='I' , predict_len = 100 , temp = 0.85):
    '''generate a new name'''
    initial_input = self.char_tensor(initial_str)
    predicted = initial_str

    hidden , cell = self.rnn.init_hidden(self.batch_size)

    for p in range(len(initial_str) - 1):
      output , (hidden , cell) = self.rnn(initial_input[p].view(1) , hidden , cell)

    last_character = initial_input[-1]

    for p in range(predict_len):
      output , (hidden , cell) = self.rnn(last_character.view(1) , hidden , cell)
      output_dist = output.data.view(-1).div(temp).exp()
      top_charac = torch.multinomial(output_dist, 1)[0]
      predicted_char = all_characters[top_charac]
      predicted += predicted_char
      last_character = self.char_tensor(predicted_char)

    print(predicted)


model = Generator()
model.train()
# model.char_tensor('I am a student')[3].shape --> 0 dimension , model.char_tensor('I am a student')[3].view(1).shape --> 1 dimention



